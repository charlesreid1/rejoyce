{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Ulysses with NLTK: Lestrygonians (Ch. 8)\n",
    "\n",
    "## Part I: Characters and Words\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "### Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Exploring the Text](#exploring)\n",
    "* Characters:\n",
    " * [Character Frequencies](#charfreq)\n",
    " * [Comparing Character Frequencies to English](#comparing_charfreq)\n",
    " * [Bigrams](#bigrams)\n",
    " * [n-grams](#ngrams)\n",
    " * [Creating an Index of n-grams](#ngram_index)\n",
    "* Words:\n",
    " * [Words words words](#www)\n",
    " * [Part of speech](#pos)\n",
    " * [Patterns in parts of speech](#pos_patterns)\n",
    " * [Improving part of speech tagging](#pos_improving)\n",
    " \n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll take the Natural Language Toolkit (NLTK) for a spin, and apply it to James Joyce's _Ulysses_ to see how _Ulysses_ is assembled - in particular, Chapter 8, which Joyce called \"Lestrygonians.\" We'll start from the lowest level - the individual letters - and work our way upwards, to n-grams, then words, then phrases, sentences, and paragraphs. To do this, we'll be using NLTK's built-in functions to tokenize, parse, clean, and process text.\n",
    "\n",
    "We'll start by importing some libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case we want to plot something:\n",
    "%matplotlib inline \n",
    "\n",
    "from __future__ import division\n",
    "import nltk, re\n",
    "\n",
    "# The io module makes unicode easier to deal with\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NLTK's built-in word tokenizer to tokenize the text. There are many tokenizers available, both at the word and sentence level, but the `word_tokenize()` function is the no-hassle option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Words, words, words.\n",
    "# Start by splitting the text into word tokens.\n",
    "\n",
    "# Create a word tokenizer object.\n",
    "# This uses io.open() because that deals with unicode more gracefully.\n",
    "tokens = nltk.word_tokenize(io.open('txt/08lestrygonians.txt','r').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tokens` is now a list containing all the word-level tokens that resulted from `word_tokenize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "15153\n"
     ]
    }
   ],
   "source": [
    "print type(tokens)\n",
    "print len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Pineapple', u'rock', u',', u'lemon', u'platt', u',', u'butter', u'scotch', u'.', u'A', u'sugarsticky', u'girl', u'shovelling', u'scoopfuls', u'of', u'creams', u'for', u'a', u'christian', u'brother', u'.']\n"
     ]
    }
   ],
   "source": [
    "print tokens[:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"exploring\"></a>\n",
    "## Exploring the Text\n",
    "\n",
    "Now that we have all the words from this chapter in a list, we can create a new object of class Text. This is a wrapper around a sequence of tokens, and is designed to explore the text by counting, providing a concordance, etc. We'll create a Text object from the list of tokens that resulted from `word_tokenize()`. The Text object also has some useful functions like `findall()`, to search for particular words or phrases using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "with meat and drink; with gold and still; with chummies and\n",
      "streetwalkers; with porringers and tommycans; with lemon and rice;\n",
      "with such and such\n",
      "--------------------\n",
      "as big as a collie; as witty as calling him; as big as the Phoenix; as\n",
      "close as damn it\n",
      "--------------------\n",
      "no more about; no go in; no teeth to; no straight sport; no June has;\n",
      "no ar no; no yes or\n",
      "--------------------\n",
      "out with the things; communicate with the outside; dress with the\n",
      "braided; was with the red; supper with the Chutney; it with the hot;\n",
      "Clerk with the glasses; meet with the approval; out with the Ward; all\n",
      "with the job; walk with the band; weather with the chill; Riordan with\n",
      "the rumbling; outs with the watch\n",
      "--------------------\n",
      "see him on; see the bluey; see them do; see the brewery; see it now;\n",
      "see her in; see anything of; see produces the; see them library; see\n",
      "if she; see a gentleman; see him look; see what he; see you across;\n",
      "see the lines; see me perhaps\n"
     ]
    }
   ],
   "source": [
    "def p():\n",
    "    print \"-\"*20\n",
    "\n",
    "# Start by creating an NLTK text object\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "p()\n",
    "text.findall(r'<with> <.*> <and> <.*>')\n",
    "p()\n",
    "text.findall(r'<as> <.*> <as> <.*> <.*>')\n",
    "p()\n",
    "text.findall(r'<no> <\\w+> <\\w+>')\n",
    "p()\n",
    "text.findall(r'<\\w+> <with> <the> <\\w+>')\n",
    "p()\n",
    "text.findall(r'<see> <\\w+> <\\w+>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Text object also provides a concordance. When a word is passed to the concordance, it prints a line of context around each occurrence of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Displaying 7 of 7 matches:\n",
      "ls writing something catch the eye at once . Everyone dying to kn\n",
      " things . Stick it in a chap’s eye in the tram . Rummaging . Open\n",
      " so older than Molly . See the eye that woman gave her , passing \n",
      "eher he has Harvey Duff in his eye . Like that Peter or Denis or \n",
      "him . Freeze them up with that eye of his . That’s the fascinatio\n",
      "bling to the left . Mr Bloom’s eye followed its line and saw agai\n",
      ". Kind of a form in his mind’s eye . The voice , temperatures : w\n",
      "--------------------\n",
      "Displaying 10 of 10 matches:\n",
      "ering themselves in and out . Molly tasting it , her veil up . Si\n",
      "us . Milly was a kiddy then . Molly had that elephantgrey dress w\n",
      " —No use complaining . How is Molly those times ? Haven’t seen he\n",
      " Only a year or so older than Molly . See the eye that woman gave\n",
      "im . Goodbye . Remember me to Molly , won’t you ? —I will , Mr Bl\n",
      " . Kill me that would . Lucky Molly got over hers lightly . They \n",
      "ogether , their bellies out . Molly and Mrs Moisel . Mothers’ mee\n",
      "s gives a woman clumsy feet . Molly looks out of plumb . He passe\n",
      "rier in the City Arms hotel . Molly fondling him in her lap . O ,\n",
      " of those silk petticoats for Molly , colour of her new garters .\n",
      "--------------------\n",
      "Displaying 13 of 13 matches:\n",
      "d you ever hear such an idea ? Eat you out of house and home . No\n",
      "tnutmeal it tastes like that . Eat pig like pig . But then why is\n",
      "weggebobbles and fruit . Don’t eat a beefsteak . If you do the ey\n",
      "ke street . Here we are . Must eat . The Burton . Feel better the\n",
      "nt . His gorge rose . Couldn’t eat a morsel here . Fellow sharpen\n",
      "w sharpening knife and fork to eat all before him , old chap pick\n",
      "the fidgets to look . Safer to eat from his three hands . Tear it\n",
      " back towards Grafton street . Eat or be eaten . Kill ! Kill ! Su\n",
      "ese . Slaughter of innocents . Eat drink and be merry . Then casu\n",
      "s out of the ground the French eat , out of the sea with bait on \n",
      "sburgs ? Or who was it used to eat the scruff off his own head ? \n",
      "gs of the flesh . Know me come eat with me . Royal sturgeon high \n",
      "wants job . Small wages . Will eat anything . Mr Bloom turned at \n"
     ]
    }
   ],
   "source": [
    "p()\n",
    "text.concordance('eye',width=65)\n",
    "p()\n",
    "text.concordance('Molly',width=65)\n",
    "p()\n",
    "text.concordance('eat',width=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joyce's _Ulysses_ is filled with colors. If we have a list of colors, we can pass them to the concordence one at a time, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Displaying 4 of 4 matches:\n",
      " and snapped the catch . Same blue serge dress she had two years\n",
      "Breen in skimpy frockcoat and blue canvas shoes shuffled out of \n",
      " , with wadding in her ears . Blue jacket and yellow cap . Bad l\n",
      "eating eggs fifty years old , blue and green again . Dinner of t\n",
      "--------------------\n",
      "Displaying 1 of 1 matches:\n",
      "No sound . The sky . The bay purple by the Lion’s head . Green b\n",
      "--------------------\n",
      "Displaying 7 of 7 matches:\n",
      " Sitting on his throne sucking red jujubes white . A sombre Y.M.C\n",
      " Hy Franks . Didn’t cost him a red like Maginni the dancing maste\n",
      " little room that was with the red wallpaper . Dockrell’s , one a\n",
      "time flies , eh ? Showing long red pantaloons under his skirts . \n",
      "th . More power , Pat . Coarse red : fun for drunkards : guffaw a\n",
      "sewage they feed on . Fizz and Red bank oysters . Effect on the s\n",
      "ual . Aphrodis . He was in the Red Bank this morning . Was he oys\n",
      "--------------------\n",
      "Displaying 6 of 6 matches:\n",
      "rg up in the trees near Goose green playing the monkeys . Mackere\n",
      "mustard , the feety savour of green cheese . Sips of his wine soo\n",
      "gs fifty years old , blue and green again . Dinner of thirty cour\n",
      "iare . Do the grand . Hock in green glasses . Swell blowout . Lad\n",
      "y purple by the Lion’s head . Green by Drumleck . Yellowgreen tow\n",
      " his teeth smooth . Something green it would have to be : spinach\n",
      "--------------------\n",
      "Displaying 9 of 9 matches:\n",
      "is throne sucking red jujubes white . A sombre Y.M.C.A . young ma\n",
      "et letters on their five tall white hats : H. E. L. Y. S. Wisdom \n",
      "mping the busk of her stays : white . Swish and soft flop her sta\n",
      "faw and smoke . Take off that white hat . His parboiled eyes . Wh\n",
      "ck feet that woman has in the white stockings . Hope the rain muc\n",
      "s would with lemon and rice . White missionary too salty . Like p\n",
      " said . —Nothing in black and white , Nosey Flynn said . Paddy Le\n",
      "black . Then passing over her white skin . Different feel perhaps\n",
      "ent feel perhaps . Feeling of white . Postoffice . Must answer . \n",
      "--------------------\n",
      "Displaying 4 of 4 matches:\n",
      "kard . Well up : it splashed yellow near his boot . A diner , kn\n",
      "dded under each lifted strip yellow blobs . Their lives . I have\n",
      "n her ears . Blue jacket and yellow cap . Bad luck to big Ben Do\n",
      "lly . But I know it’s whitey yellow . Want to try in the dark to\n"
     ]
    }
   ],
   "source": [
    "colors = ['blue','purple','red','green','white','yellow']#'indigo','violet']\n",
    "for c in colors:\n",
    "    p()\n",
    "    text.concordance(c,width=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 8 of _Ulysses_, Lestrygonians, is named for the episode in Homer's _Odyssey_ in which Odysseus and his crew encounter the island of the cannibal Lestrygonians. The language in the chapter very much reflects that. Here we use the count method of the Text class to count some meaty, sensory, organic words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"eyes\" : 30\n",
      "\"eye\" : 7\n",
      "\"mouth\" : 14\n",
      "\"God\" : 10\n",
      "\"food\" : 10\n",
      "\"eat\" : 9\n",
      "\"knife\" : 7\n",
      "\"blood\" : 7\n",
      "\"son\" : 3\n",
      "\"teeth\" : 2\n",
      "\"skin\" : 3\n",
      "\"meat\" : 6\n",
      "\"flies\" : 3\n",
      "\"guts\" : 2\n"
     ]
    }
   ],
   "source": [
    "# Count a few words\n",
    "def count_it(word):\n",
    "    print '\"'+word+'\" : ' + str(text.count(word))\n",
    "\n",
    "words = ['eyes','eye','mouth','God','food','eat','knife','blood','son','teeth','skin','meat','flies','guts']\n",
    "for w in words:\n",
    "    count_it(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the results of `word_tokenize()` again, which were stored in the `tokens` variable. This contained about 15,000 words. We can use this as an all-inclusive wordlist, and filter out words based on certain criteria to get new wordlists. We can also use built-in methods for strings to process words. Here are two examples: one that converts all tokens to lowercase using a built-in method for strings, and one that extracts words with the suffix \"-ed\" using a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The tokens variable is a list containing \n",
    "# a full wordlist, plus punctuation.\n",
    "# \n",
    "# We can make word lists by filtering out words based on criteria.\n",
    "# Can use regular expressions or built-in string methods.\n",
    "\n",
    "# For example, a list of all lowercase words using \n",
    "# built-in string methods to filter:\n",
    "#lowerlist = [w for w in tokens if w.islower()]\n",
    "lowerlist = [w.lower() for w in tokens]\n",
    "\n",
    "# and use this to find all \"-ed\" suffixes\n",
    "# using a regular expression:\n",
    "verbed = [w2 for w2 in lowerlist if re.search('ed$',w2) and len(w2)>4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allows us to do things like compile lists of words that are either colors themselves, or that have color words in them. To find unique words, we use a set object, a built-in type for Python, and add words that contain color words ('blue','green', and so on). The color red tends to add lots of non-color words so is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'blue', u'greenhouses', u'greens', u'penrose', u'orangepeels', u'bluecoat', u'orangegroves', u'greeny', u'yellow', u'bluey', u'yellowgreen', u'green', u'rose', u'blues', u'greenwich'])\n"
     ]
    }
   ],
   "source": [
    "colors = ['orange','yellow','green','blue','indigo','rose','violet']#,'red']\n",
    "mecolors = set()\n",
    "_ = [mecolors.add(w.lower()) for c in colors for w in tokens if re.search(c,w.lower()) ]\n",
    "print mecolors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"charfreq\"></a>\n",
    "## Character Frequencies\n",
    "\n",
    "To count character frequencies, we'll use a character iterator within a word iterator, and turn it loose on the entire chapter. The list containing each word converted to lowercase, `lowerlist`, will come in handy for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char. : occurr. : freq.\n",
      "a : 4063 times : 7.39 %\n",
      "c : 1230 times : 2.24 %\n",
      "b : 0995 times : 1.81 %\n",
      "e : 6448 times : 11.73 %\n",
      "d : 2215 times : 4.03 %\n",
      "g : 1438 times : 2.62 %\n",
      "f : 1296 times : 2.36 %\n",
      "i : 3700 times : 6.73 %\n",
      "h : 3355 times : 6.10 %\n",
      "k : 0701 times : 1.28 %\n",
      "j : 0090 times : 0.16 %\n",
      "m : 1487 times : 2.71 %\n",
      "l : 2561 times : 4.66 %\n",
      "o : 4347 times : 7.91 %\n",
      "n : 3616 times : 6.58 %\n",
      "q : 0056 times : 0.10 %\n",
      "p : 1079 times : 1.96 %\n",
      "s : 3799 times : 6.91 %\n",
      "r : 3160 times : 5.75 %\n",
      "u : 1592 times : 2.90 %\n",
      "t : 4737 times : 8.62 %\n",
      "w : 1260 times : 2.29 %\n",
      "v : 0410 times : 0.75 %\n",
      "y : 1234 times : 2.25 %\n",
      "x : 0052 times : 0.09 %\n",
      "z : 0045 times : 0.08 %\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary that contains total count of each character\n",
    "charcount = {}\n",
    "tot = 0\n",
    "for word in lowerlist:\n",
    "    for ch in word:\n",
    "        if ch in charcount.keys():\n",
    "            charcount[ch] += 1\n",
    "            tot += 1\n",
    "        elif (re.match('^[A-Za-z]{1,}$', ch) is not None):\n",
    "            charcount[ch] = 1\n",
    "            tot += 1\n",
    "\n",
    "# Make a dictionary that contains frequency of each character\n",
    "charfrequencies = {}\n",
    "keys = charcount.keys()\n",
    "keys.sort()\n",
    "for k in keys:\n",
    "    f = charcount[k]/(1.0*tot)\n",
    "    charfrequencies[k] = f*100\n",
    "\n",
    "print \"%s : %s : %s\"%('char.','occurr.','freq.')\n",
    "for k in charcount.keys():\n",
    "    print \"%s : %04d times : %0.2f %%\"%(k,charcount[k],charfrequencies[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"comparing_charfreq\"></a>\n",
    "## Comparing Character Frequencies to English\n",
    "\n",
    "Using values of character frequencies for a broader sample of the language spectrum, we can determine how closely the use of the alphabet in Lestrygonians matches the usage of the alphabet in common language, and whether there's a there there. We'll start by importing a dictionary that contains a key/value set of frequencies for each letter.\n",
    "\n",
    "The following big of code makes sure that we're only dealing with the alphabet, and that the keys match between the Lestrygonians character frequency dictionary and the English language character frequency dictionary that we just imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from English import EnglishLanguageLetterFrequency\n",
    "ufk = set([str(k) for k in charfrequencies.keys()])\n",
    "efk = set([str(k) for k in EnglishLanguageLetterFrequency.keys()])\n",
    "common_keys = ufk.intersection(efk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute how large of a deviation from \"normal\" the frequency of this character is. We use the formula for percent difference. If the character frequency of a given character is given by $y$, and the character frequency of that character in the Lestrygonians chapter is denoted $y_L$ and the character frequency of that character in the English language is given by $y_{E}$, then the formula for percent difference is given by:\n",
    "\n",
    "$$\n",
    "\\text{Pct Diff} = \\dfrac{y_{L} - y_{E}}{y_{E}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_variation = {}\n",
    "for k in common_keys:\n",
    "    clf = charfrequencies[k]\n",
    "    elf = EnglishLanguageLetterFrequency[k]\n",
    "    pd = ((clf-elf)/elf)*100\n",
    "    frequency_variation[k] = pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : -8.97 %\n",
      "c : -17.43 %\n",
      "b : 21.49 %\n",
      "e : -2.41 %\n",
      "d : -6.72 %\n",
      "g : 28.88 %\n",
      "f : 2.51 %\n",
      "i : -7.91 %\n",
      "h : 3.10 %\n",
      "k : 84.83 %\n",
      "j : 63.74 %\n",
      "m : 3.65 %\n",
      "l : 17.07 %\n",
      "o : 2.98 %\n",
      "n : -5.34 %\n",
      "q : -7.38 %\n",
      "p : 7.86 %\n",
      "s : 10.06 %\n",
      "r : -4.50 %\n",
      "u : 0.57 %\n",
      "t : -5.30 %\n",
      "w : 9.68 %\n",
      "v : -32.80 %\n",
      "y : 6.40 %\n",
      "x : -44.35 %\n",
      "z : 16.96 %\n"
     ]
    }
   ],
   "source": [
    "for k in frequency_variation.keys():\n",
    "    print \"%s : %0.2f %%\"%(k,frequency_variation[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see (and would expect) that there is more variation among the less common letters (higher sensitivity to variation). \n",
    "\n",
    "However, there are some meaningful trends - if we pick out the letters that have a large positive percent difference, meaning they occur more in this chapter than they usually do in English, we find more harsh sounds: there are more b's, g's, j's and k's in this chapter than would be expected. These sounds are more guttural, cutting, blubbering letters, and fit with the character of Lestrygonians. Joyce's word selection in this chapter comes through even at the character frequency level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bigrams\"></a>\n",
    "## Bigrams\n",
    "\n",
    "The next thing we can do is use regular expressions to analyze the bigrams that appear in the chapter. Regular expressions allow us to seek out pairs of letters matching a particular pattern. Combined with list comprehensions, we can do some very powerful analysis with just a little bit of code. \n",
    "\n",
    "For example, this one-liner iterates over each word token, grabs bigrams matching a particular regular expression, and uses the result to initialize a frequency distribution object, which enables quick and easy access to useful statistical information about the bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vowel_bigrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[aeiou]{2}',word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression syntax `'[aeiou]'` will match any vowels, and the `'{2}'` syntax means, match exactly 2 occurrences of the pattern (two vowels in a row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ou', 554),\n",
       " (u'ea', 366),\n",
       " (u'oo', 304),\n",
       " (u'ee', 265),\n",
       " (u'ai', 229),\n",
       " (u'ie', 91),\n",
       " (u'io', 75),\n",
       " (u'ei', 65),\n",
       " (u'oa', 55),\n",
       " (u'oi', 51),\n",
       " (u'ui', 49),\n",
       " (u'oe', 46),\n",
       " (u'au', 44),\n",
       " (u'ue', 39),\n",
       " (u'eo', 36),\n",
       " (u'ia', 34),\n",
       " (u'ua', 25),\n",
       " (u'aa', 7),\n",
       " (u'eu', 6),\n",
       " (u'ii', 3),\n",
       " (u'ae', 2),\n",
       " (u'uu', 1),\n",
       " (u'uo', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vowel_bigrams.items(), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far the most common vowel-vowel bigram in Chapter 8 is \"ou\", with 554 occurrences, followed by \"ea\" (366 occurrences) and \"oo\" (304 occurrences) in a distant second and third place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'in', 1157),\n",
       " (u'er', 850),\n",
       " (u'an', 623),\n",
       " (u'on', 540),\n",
       " (u'at', 516),\n",
       " (u'ed', 478),\n",
       " (u'es', 476),\n",
       " (u'ar', 455),\n",
       " (u'is', 439),\n",
       " (u'it', 429),\n",
       " (u'or', 414),\n",
       " (u'en', 392),\n",
       " (u'of', 346),\n",
       " (u'as', 326),\n",
       " (u'al', 322),\n",
       " (u'el', 285),\n",
       " (u'om', 277),\n",
       " (u'ow', 253),\n",
       " (u'et', 237),\n",
       " (u'ut', 221)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_bigrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[aeiou][^aeiou]',word))\n",
    "sorted(vc_bigrams.items(), key=lambda tup: tup[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'th', 1402),\n",
       " (u'ng', 631),\n",
       " (u'st', 452),\n",
       " (u'nd', 438),\n",
       " (u'll', 420),\n",
       " (u'nt', 210),\n",
       " (u'sh', 208),\n",
       " (u'rs', 172),\n",
       " (u'gh', 161),\n",
       " (u'ch', 161),\n",
       " (u'ck', 159),\n",
       " (u'ld', 151),\n",
       " (u'bl', 144),\n",
       " (u'wh', 141),\n",
       " (u'ss', 132),\n",
       " (u'rd', 113),\n",
       " (u'rt', 112),\n",
       " (u'ns', 111),\n",
       " (u'tt', 108),\n",
       " (u'rn', 98)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_bigrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[^aeiou]{2}',word))\n",
    "cc_bigrams.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More guttural, slicing, cutting sounds populate the bigrams appearing in this chapter: \"in\", \"er\", \"an\", \"on\", \"at\" dominate the consonant-vowel bigrams, while \"th\", \"ng\", \"st\", \"nd\", and \"ll\" dominate the consonant-consonant bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a tabulated bigram table - this requires a different type of frequency distribution. We can create a conditional frequency distribution, which tabulates information like \"what is the frequency of the second letter, conditional on the value of the first letter?\" Create a conditional frequency distribution and use the `tabulate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u \n",
      "-    1    0    1    6    0 \n",
      ".    1    0    0    0    0 \n",
      "b  137  207   53  120   76 \n",
      "c  144  156   38  229   43 \n",
      "d  116  222  119  145   41 \n",
      "f   68  133  106  214   40 \n",
      "g   93  177   61  138   43 \n",
      "h  521 1476  486  251   53 \n",
      "j   17    9    4   26   32 \n",
      "k    7  218  116    1    6 \n",
      "l  196  340  306  290   60 \n",
      "m  189  358  108  153   68 \n",
      "n   76  326  130  233   32 \n",
      "p  124  156   97  128   61 \n",
      "q    0    0    0    0   55 \n",
      "r  180  608  221  269   65 \n",
      "s  181  419  157  159   76 \n",
      "t  178  385  246  451   71 \n",
      "v   21  306   39   18    0 \n",
      "w  293  147  214  149    0 \n",
      "x    7    7    1    1    1 \n",
      "y    7  112   20  153    3 \n",
      "z    2   22    8    1    0 \n",
      "—   12    0   19   10    2 \n",
      "‘    0    1    0    0    0 \n",
      "’    2    2    1    0    0 \n"
     ]
    }
   ],
   "source": [
    "cvs = [cv for w in lowerlist for cv in re.findall(r'[^aeiou][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ngrams\"></a>\n",
    "# n-grams\n",
    "\n",
    "We can also look at some n-grams - combinations of n letters. Let's start by revisiting our search for two vowels, and modify the regular expression we used to look for three or more vowels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'uie', 7),\n",
       " (u'iou', 7),\n",
       " (u'eei', 6),\n",
       " (u'uee', 5),\n",
       " (u'eau', 4),\n",
       " (u'iiiiii', 1),\n",
       " (u'ieu', 1),\n",
       " (u'uea', 1),\n",
       " (u'aaaaaa', 1),\n",
       " (u'oooi', 1),\n",
       " (u'aaaaaaa', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel_ngrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[aeiou]{3,}',word))\n",
    "vowel_ngrams.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for consonant-vowel-vowel or vowel-vowel-consonant combinations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'you', 140),\n",
       " (u'loo', 98),\n",
       " (u'ree', 92),\n",
       " (u'sai', 79),\n",
       " (u'rea', 71),\n",
       " (u'hea', 57),\n",
       " (u'too', 54),\n",
       " (u'see', 53),\n",
       " (u'cou', 53),\n",
       " (u'rou', 48),\n",
       " (u'hou', 43),\n",
       " (u'tio', 41),\n",
       " (u'hei', 37),\n",
       " (u'goo', 36),\n",
       " (u'bou', 35),\n",
       " (u'rai', 30),\n",
       " (u'lea', 29),\n",
       " (u'yea', 27),\n",
       " (u'fee', 26),\n",
       " (u'qui', 25)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvv_ngrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[^aeiou][aeiou]{2}',word))\n",
    "cvv_ngrams.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'out', 137),\n",
       " (u'ear', 96),\n",
       " (u'our', 87),\n",
       " (u'aid', 83),\n",
       " (u'eat', 70),\n",
       " (u'ood', 69),\n",
       " (u'ain', 68),\n",
       " (u'oun', 62),\n",
       " (u'ion', 62),\n",
       " (u'eet', 57),\n",
       " (u'oul', 55),\n",
       " (u'ook', 55),\n",
       " (u'ead', 53),\n",
       " (u'oom', 52),\n",
       " (u'ous', 42),\n",
       " (u'oug', 41),\n",
       " (u'een', 36),\n",
       " (u'eir', 36),\n",
       " (u'ies', 29),\n",
       " (u'eas', 29)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vvc_ngrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[aeiou]{2}[^aeiou]',word))\n",
    "vvc_ngrams.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ere', 119),\n",
       " (u'ose', 94),\n",
       " (u'ome', 94),\n",
       " (u'one', 88),\n",
       " (u'ave', 81),\n",
       " (u'are', 64),\n",
       " (u'ine', 61),\n",
       " (u'ike', 59),\n",
       " (u'eve', 54),\n",
       " (u'ate', 53),\n",
       " (u'ive', 53),\n",
       " (u'use', 50),\n",
       " (u'ake', 46),\n",
       " (u'ove', 46),\n",
       " (u'eye', 44),\n",
       " (u'ame', 42),\n",
       " (u'ime', 38),\n",
       " (u'ide', 37),\n",
       " (u'ite', 35),\n",
       " (u'ice', 35)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcv_ngrams = nltk.FreqDist(vs for word in lowerlist for vs in re.findall(r'[aeiou][^aeiou][aeiou]',word))\n",
    "vcv_ngrams.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting results - \"you\", \"out\", and \"ere\" are the most common combinations of two vowels and a consonant, followed by \"loo\", \"ear\", \"ose\", and \"ome\". Plenty of o's in those syllables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ngram_index\"></a>\n",
    "## Creating an Index of n-grams\n",
    "\n",
    "It's interesting to pick up on some of the patterns in bigrams and n-grams that occur in Lestrygonians, but how can we connect this better to the text itself? This is still a bit too abstract. \n",
    "\n",
    "NLTK provides an Index object, which allows you to create a custom index with a list of words. Much like an index collects a variety of subjects under letter headings, and subject sub-headings, an Index object allows you to group information, but more powerfully - based on a custom criteria. This is similar to a Python dictionary type.\n",
    "\n",
    "In this case, we'll cluster words based on the n-grams that they contain, so that we can look up all the words that contain a particular n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words were found using the bigram index.\n",
      "The words are listed in order of appearance in the text.\n",
      "--------------------\n",
      "Bigram: pp\n",
      "[u'pineapple', u'stopper', u'pepper\\u2019s', u'kippur', u'flapping', u'flapping', u'apples', u'apples', u'applewoman', u'flapping', u'appetite', u'suppose', u'dripping', u'lapping', u'happy', u'happier', u'stopped', u'supperroom', u'appearance', u'supper', u'happy', u'happy', u'chipped', u'snapped', u'gripped', u'approval', u'sloppy', u'supposed', u'suppose', u'happy', u'sourapple', u'opposite', u'apply', u'approval', u'pineapple', u'happier', u'happy', u'gripped', u'sloppy', u'sopping', u'sippets', u'suppose', u'souppot', u'kippur', u'sipped', u'smellsipped', u'kipper', u'pepper', u'ripped', u'dropping', u'nipples', u'suppose', u'sipping', u'hiccupped', u'lapped', u'upper', u'supper', u'tapping', u'opposite', u'suppose', u'tapped', u'tapping', u'suppose', u'quopped']\n",
      "--------------------\n",
      "Bigram: bb\n",
      "[u'lobbing', u'bobbed', u'bobbob', u'rabbitpie', u'tubbing', u'shabby', u'stubbs', u'abbey', u'cobblestones', u'rubble', u'weggebobbles', u'ribbons', u'cabbage', u'cabbage', u'cabbage', u'bubble', u'wobbly', u'rabbi', u'cabbage', u'pebbles', u'dribbling', u'cobblestones', u'slobbers']\n",
      "--------------------\n",
      "Bigram: pu\n",
      "[u'put', u'kippur', u'pudding', u'puffball', u'puke', u'purchase', u'put', u'jampuffs', u'pungent', u'purefoy', u'pull', u'popular', u'put', u'pugnosed', u'purefoy', u'put', u'public', u'pumpkin', u'pudding', u'punch', u'put', u'pupil', u'purefoy', u'squarepushing', u'putting', u'republicanism', u'purefoy', u'put', u'puffed', u'public', u'put', u'octopus', u'octopus', u'homespun', u'pursue', u'purty', u'pursued', u'pushed', u'pungent', u'pub', u'puzzle', u'kippur', u'puts', u'pure', u'putting', u'pub', u'put', u'pungent', u'put', u'putting', u'pulse', u'purple', u'pulp', u'put', u'pursed', u'put', u'publichouse', u'pulling', u'pulled', u'purse', u'put']\n",
      "--------------------\n",
      "Bigram: ki\n",
      "[u'king', u'sucking', u'kidney', u'kitchen', u'thinking', u'kippur', u'drinking', u'looking', u'looking', u'king', u'kind', u'kino\\u2019s', u'kinds', u'thinking', u'walking', u'skin', u'skilly', u'kiddy', u'looking', u'priestylooking', u'linking', u'skirts', u'raking', u'taking', u'talking', u'walking', u'taking', u'taking', u'thinking', u'kids', u'skimpy', u'drinking', u'thinking', u'kill', u'pumpkin', u'knocking', u'making', u'looking', u'talking', u'buckingham', u'kicked', u'walking', u'dickinson', u'taking', u'stockings', u'kind', u'making', u'thinking', u'looking', u'king\\u2019s', u'walking', u'asking', u'walking', u'kinsella', u'skirts', u'drinking', u'baking', u'stockings', u'sticking', u'stockings', u'kissed', u'creaking', u'\\u2014kiss', u'napkin', u'napkin', u'working', u'king', u'picking', u'kill', u'kill', u'kitchen', u'drinkingcup', u'smokinghot', u'looking', u'kippur', u'kipper', u'sucking', u'napkin', u'taking', u'kissing', u'mawkish', u'kindled', u'kitchen', u'killiney', u'making', u'moooikill', u'kish', u'kissed', u'mawkish', u'walking', u'kissed', u'kissed', u'kissed', u'kissed', u'kissed', u'speaking', u'drinking', u'stoking', u'taking', u'drinking', u'suckingbottle', u'kilkenny', u'slaking', u'kind', u'kind', u'skin', u'skin', u'walking', u'falkiner', u'cracking', u'sticking', u'kildare', u'making', u'looking', u'looking', u'looking']\n"
     ]
    }
   ],
   "source": [
    "cc_ngram = [(cc,w) for w in lowerlist for cc in re.findall('[^aeiou]{2}',w)]\n",
    "cc_index = nltk.Index(cc_ngram)\n",
    "\n",
    "cv_ngram = [(cv,w) for w in lowerlist for cv in re.findall('[^aeiou][aeiou]',w)]\n",
    "cv_index = nltk.Index(cv_ngram)\n",
    "\n",
    "print \"The following words were found using the bigram index.\"\n",
    "print \"The words are listed in order of appearance in the text.\"\n",
    "\n",
    "p()\n",
    "print \"Bigram: pp\"\n",
    "print cc_index['pp']\n",
    "p()\n",
    "print \"Bigram: bb\"\n",
    "print cc_index['bb']\n",
    "p()\n",
    "print \"Bigram: pu\"\n",
    "print cv_index['pu']\n",
    "p()\n",
    "print \"Bigram: ki\"\n",
    "print cv_index['ki']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we revisit the most common trigrams, we can see which words cuased those trigrams to appear so often. Take the \"you\" trigram, which was the most common at 140 occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words containing trigram 'you': 140\n",
      "Number of unique words containing trigram 'you': 12\n",
      "They are:\n",
      "him—you\n",
      "—you’re\n",
      "you’ve\n",
      "you’ll\n",
      "youths\n",
      "young\n",
      "you’re\n",
      "youth\n",
      "you’d\n",
      "you\n",
      "yours\n",
      "your\n"
     ]
    }
   ],
   "source": [
    "cvc_ngram = [(cv,w) for w in lowerlist for cv in re.findall(r'[^aeiou][aeiou][aeiou]',w)]\n",
    "cvc_index = nltk.Index(cvc_ngram)\n",
    "\n",
    "print \"Number of words containing trigram 'you': \" + str( len(cvc_index['you']) )\n",
    "print \"Number of unique words containing trigram 'you': \" + str( len(set(cvc_index['you'])) )\n",
    "print \"They are:\"\n",
    "for t in set(cvc_index['you']):\n",
    "    print t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also more convenient to create an Index that looks at all trigrams, instead of just consonant-vowel-consonant or vowel-consonant-vowel. Modifying the regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Number of words containing trigram 'one': 53\n",
      "Number of unique words containing trigram 'one': 9\n",
      "They are:\n",
      "                             one :   35\n",
      "                          no-one :    5\n",
      "                       curbstone :    5\n",
      "                            —one :    2\n",
      "                       twentyone :    2\n",
      "                          throne :    1\n",
      "                       woebegone :    1\n",
      "                       funnybone :    1\n",
      "                            ones :    1\n",
      "--------------------\n",
      "Number of words containing trigram 'can': 30\n",
      "Number of unique words containing trigram 'can': 8\n",
      "They are:\n",
      "                             can :   11\n",
      "                           can’t :    9\n",
      "                            cane :    5\n",
      "                          canvas :    1\n",
      "                       cannibals :    1\n",
      "                          cannon :    1\n",
      "                           canny :    1\n",
      "                      canvassing :    1\n"
     ]
    }
   ],
   "source": [
    "aaa_ngram = [(cv,w) for w in lowerlist for cv in re.findall(r'[A-Za-z]{3}',w)]\n",
    "aaa_index = nltk.Index(aaa_ngram)\n",
    "\n",
    "def print_trigram(trigram):\n",
    "    print \"Number of words containing trigram '\"+trigram+\"': \" + str( len(aaa_index[trigram]) )\n",
    "    print \"Number of unique words containing trigram '\" + trigram + \"': \" + str( len(set(aaa_index[trigram])) )\n",
    "    print \"They are:\"\n",
    "    trigram_fd = nltk.FreqDist(aaa_index[trigram])    \n",
    "    for mc in trigram_fd.most_common(): \n",
    "        print \"%32s : %4d\"%(mc[0],mc[1])\n",
    "\n",
    "p()\n",
    "print_trigram('one')\n",
    "\n",
    "p()\n",
    "print_trigram('can')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"www\"></a>\n",
    "## Words, words, words\n",
    "\n",
    "NLTK also provides tools for exploring the text at the word level. Let's start by looking at word lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean word size: 3.8366000132\n",
      "Std. dev. in word size: 2.37301284681\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sizes = [len(word) for word in lowerlist]\n",
    "mean_size = np.mean(sizes)\n",
    "var_size = np.std(sizes)\n",
    "print \"Mean word size: \" + str(mean_size)\n",
    "print \"Std. dev. in word size: \" + str(var_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compose Lestrygonians, Joyce's choice of words tended toward words containing more harsh consonants like b's, g's, j's, and k's; shorter word lengths (in keeping with the peristaltic nature of the chapter's language, masticating words, passing them down the mental esophagus one bit at a time (2-6 letters).\n",
    "\n",
    "What about the percentage of vowels and consonants occurring in the text?\n",
    "\n",
    "This should be possible with a one liner. We'll have a nested for loop - one loop over all lowercase word tokens, and one over all letters in the word token. Then we'll count up the number of matches of exactly 1 instance of `[aeiou]`  and the number of matches of exactly 1 instance of letters that are not `aeiou` (note that `[^aeiou]` will also count punctuation, we need to be careful), and that will give us our letter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lestrygonians contains 34816 consonants and 20150 vowels, ratio of vowels:consonants is 0.58\n"
     ]
    }
   ],
   "source": [
    "# Count consonants and vowels\n",
    "consonants = [re.findall('[b-df-hj-np-tv-z]',w) for w in lowerlist]\n",
    "n_consonants = sum([len(e) for e in consonants])\n",
    "vowels = [re.findall('[aeiou]',w) for w in lowerlist]\n",
    "n_vowels = sum([len(e) for e in vowels])\n",
    "\n",
    "print \"Lestrygonians contains %d consonants and %d vowels, ratio of vowels:consonants is %0.2f\"%(n_consonants,n_vowels,n_vowels/n_consonants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that interesting phenomena, whereby text is still partly or mostly comprehensible, even with all of the vowels removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pnppl rck  lmn pltt  bttr sctch   sgrstcky grl shvllng scpfls f crms fr  chrstn brthr  sm schl trt  bd fr thr tmms  lzng nd cmft mnfctrr t hs mjsty th kng  gd  sv  r  sttng n hs thrn sckng rd jjbs wht   smbr ymc  yng mn  wtchfl mng th wrm swt fms f grhm lmns  plcd  thrwwy n  hnd f mr blm  hrt t hrt tlks  bl  m  n  bld f th lmb  hs slw ft wlkd hm rvrwrd  rdng  r y svd  ll r wshd n th bld f th lmb  gd wnts bld vctm  brth  hymn  mrtyr  wr  fndtn f  bldng  scrfc  kdny brntffrng  drds '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([''.join(t)+\" \" for t in consonants[:145]])\n",
    "#for t in consonants[:21]:\n",
    "#    print ''.join(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit that `text` object from earlier. Recall we created it using the code:\n",
    "\n",
    "```\n",
    "text = nltk.Text(tokens)\n",
    "```\n",
    "\n",
    "The Text object has made a determination of what words are similar, based on their appearance near one another in the text. We can obtain this list using the `similar()` method. The results are interesting to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run suckingbottle bull feast cheque night collation dream\n"
     ]
    }
   ],
   "source": [
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nannygoat provinces eating penny bed walk person stick chat vacuum\n",
      "marketnet bit day\n"
     ]
    }
   ],
   "source": [
    "text.similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the woman-night and man-day association, and the woman-bull and man-nannygoat association. Some curious things here...\n",
    "\n",
    "Because of the way the Text class is implemented, these are printed directly to output, instead of being returned as a list or something convenient. See [the NLTK source](http://www.nltk.org/_modules/nltk/text.html#ContextIndex.word_similarity_dict) for details. But we can fix this by extending the Text class to make our own class, called the `SaneText` class, to behave more sanely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# This block of code is extending an NLTK object\n",
    "# to modify its behavior.\n",
    "# \n",
    "# This creates a SaneText object, which modifies the similar() method\n",
    "# to behave in a sane way (return a list of words, instead of printing it\n",
    "# to the console).\n",
    "#\n",
    "# Source: NLTK source code. \n",
    "# Modified lines are indicated.\n",
    "# http://www.nltk.org/_modules/nltk/text.html#ContextIndex.word_similarity_dict\n",
    "\n",
    "from nltk.compat import Counter\n",
    "from nltk.util import tokenwrap\n",
    "\n",
    "class SaneText(nltk.Text):\n",
    "\n",
    "    def similar(self, word, num=20):\n",
    "        \"\"\"\n",
    "        This is copied and pasted from the NLTK source code,\n",
    "        but with print statements replaced with return statements.\n",
    "        \"\"\"\n",
    "\n",
    "        if '_word_context_index' not in self.__dict__:\n",
    "            #print('Building word-context index...')\n",
    "            self._word_context_index = nltk.ContextIndex(self.tokens,\n",
    "                                                    filter=lambda x:x.isalpha(),\n",
    "                                                    key=lambda s:s.lower())\n",
    "\n",
    "#        words = self._word_context_index.similar_words(word, num)\n",
    "\n",
    "        word = word.lower()\n",
    "        wci = self._word_context_index._word_to_contexts\n",
    "        if word in wci.conditions():\n",
    "            contexts = set(wci[word])\n",
    "            fd = Counter(w for w in wci.conditions() for c in wci[w]\n",
    "                          if c in contexts and not w == word)\n",
    "            words = [w for w, _ in fd.most_common(num)]\n",
    "            \n",
    "            #######\n",
    "            # begin changed lines\n",
    "            return tokenwrap(words)\n",
    "        else:\n",
    "            return u''\n",
    "            # end changed lines\n",
    "            ######\n",
    "#\n",
    "# Done defining custom class\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             you : him me all i cross soap how found bread fizz\n",
      "                           bread : things you fizz tooth\n",
      "                            beer : his\n",
      "                            food : veins on times\n",
      "                           blood : stream presence house broth church ends dead provost stings skin\n",
      "scrapings stare closes conversion gnaw top tip somethings wings busk\n",
      "                            meat : chummies lemon gold porringers such pleasure\n",
      "                           grave : lees fire menu stooled mater bench cobblestones window curbstone\n",
      "plates world river minute meet\n",
      "                           right : allusion clock hasty ballastoffice completely belly night\n",
      "                          priest : thing wit taxes\n",
      "                           devil : lamb stopper carver rum fascination spring confession gaff missus\n",
      "curves brain rightabout lines northwest way sky world left arm ground\n",
      "                            coat : babies\n",
      "                           dress : notice house remark hand pocket glass guard\n",
      "                            life : cutlet was\n",
      "                          babies : coat\n",
      "                            lamb : king devil northwest curves\n",
      "                            king : lamb\n",
      "                             cut : has ate said on\n",
      "                            fork : walked tommycans\n"
     ]
    }
   ],
   "source": [
    "sanetext = SaneText(tokens)\n",
    "\n",
    "def similar_words(w):\n",
    "    print \"%32s : %s\"%( w, sanetext.similar(w) )\n",
    "\n",
    "similar_words('you')\n",
    "similar_words('bread')\n",
    "similar_words('beer')\n",
    "similar_words('food')\n",
    "similar_words('blood')\n",
    "similar_words('meat')\n",
    "similar_words('grave')\n",
    "similar_words('right')\n",
    "similar_words('priest')\n",
    "similar_words('devil')\n",
    "similar_words('coat')\n",
    "similar_words('dress')\n",
    "similar_words('life')\n",
    "similar_words('babies')\n",
    "similar_words('lamb')\n",
    "similar_words('king')\n",
    "similar_words('cut')\n",
    "similar_words('fork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              he : she not i too morning\n",
      "                             she : he even what i morning they multiply\n",
      "                            what : would it she multiply messiah if\n",
      "                             God : see publichouse\n"
     ]
    }
   ],
   "source": [
    "similar_words('he')\n",
    "similar_words('she')\n",
    "similar_words('what')\n",
    "similar_words('God')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some curious and unexpected connections here - particularly, \"God and \"publichouse,\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"pos\"></a>\n",
    "## Part of Speech\n",
    "\n",
    "Let's tag each sentence of the text with its part of speech (POS), using NLTK's built-in method (trained on a large corpus of data). \n",
    "\n",
    "**NOTE:** It's important to be very skeptical of the part of speech tagger's results. The following does not focus on whether the parts of speech being tagged are correct, as that leads to other topics involving multiple words. Here, we show how to analyze the results of a part of speech tagger, not how to train one to be more accurate.\n",
    "\n",
    "We can tag the parts of speech with the `nltk.pos_tag()` method. This is a static method that results in a list of tuples. Each tuple represents a word and its part of speech tag. The sentence\n",
    "\n",
    "`I am Jane.`\n",
    "\n",
    "would be tagged:\n",
    "\n",
    "`I (PRON) am (VERB) Jane (NOUN) . (.)`\n",
    "\n",
    "and would be stored as the tuple:\n",
    "\n",
    "```\n",
    "[('I',    'PRON'),\n",
    " ('am',   'VERB'),\n",
    " ('Jane', 'NOUN'),\n",
    " ('.',    '.')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', u'PRON'), ('am', u'VERB'), ('Jane', u'NOUN'), ('.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "print nltk.pos_tag(['I','am','Jane','.'],tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either pass it a list (like the result of the `nltk.word_tokenize()` method, and our `tokens` variable above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "<type 'list'>\n",
      "--------------------\n",
      "[u'Pineapple', u'rock', u',', u'lemon', u'platt', u',', u'butter', u'scotch', u'.', u'A', u'sugarsticky', u'girl', u'shovelling', u'scoopfuls', u'of', u'creams', u'for', u'a', u'christian', u'brother', u'.']\n",
      "--------------------\n",
      "[(u'Pineapple', u'NOUN'), (u'rock', u'NOUN'), (u',', u'.'), (u'lemon', u'ADJ'), (u'platt', u'NOUN'), (u',', u'.'), (u'butter', u'NOUN'), (u'scotch', u'NOUN'), (u'.', u'.'), (u'A', u'DET'), (u'sugarsticky', u'ADJ'), (u'girl', u'NOUN'), (u'shovelling', u'VERB'), (u'scoopfuls', u'NOUN'), (u'of', u'ADP'), (u'creams', u'NOUN'), (u'for', u'ADP'), (u'a', u'DET'), (u'christian', u'ADJ'), (u'brother', u'NOUN'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "p()\n",
    "print type(tokens[:21])\n",
    "p()\n",
    "print tokens[:21]\n",
    "p()\n",
    "print nltk.pos_tag(tokens[:21],tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can pass it an NLTK Text object, like our `text` object above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "<class 'nltk.text.Text'>\n",
      "--------------------\n",
      "<Text: Pineapple rock , lemon platt , butter scotch...>\n",
      "--------------------\n",
      "[(u'Pineapple', u'NOUN'), (u'rock', u'NOUN'), (u',', u'.'), (u'lemon', u'ADJ'), (u'platt', u'NOUN'), (u',', u'.'), (u'butter', u'NOUN'), (u'scotch', u'NOUN'), (u'.', u'.'), (u'A', u'DET'), (u'sugarsticky', u'ADJ'), (u'girl', u'NOUN'), (u'shovelling', u'VERB'), (u'scoopfuls', u'NOUN'), (u'of', u'ADP'), (u'creams', u'NOUN'), (u'for', u'ADP'), (u'a', u'DET'), (u'christian', u'ADJ'), (u'brother', u'NOUN'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "p()\n",
    "t = nltk.Text(nltk.Text(tokens[:21]))\n",
    "print type(t)\n",
    "p()\n",
    "print t\n",
    "p()\n",
    "print nltk.pos_tag(t,tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tag the part of speech of the *entire* text, we can utilize list comprehensions, built-in string methods, and regular expressions to pick out some interesting information about parts of speech in Chapter 8. For example, we can search for verbs and include patterns to ensure a particular tense, search for nouns ending in \"s\" only, or extract parts of speech and analyze frequency distributions.\n",
    "\n",
    "We'll start by tagging the parts of speech of our entire text. Recall above our list of all tokens was stored in the variable `tokens`:\n",
    "\n",
    "```\n",
    "tokens = nltk.word_tokenize(io.open('txt/08lestrygonians.txt','r').read())\n",
    "```\n",
    "\n",
    "We can feed this to the `pos_tag()` method to get the fully-tagged text of Lestrygonians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tags = nltk.pos_tag(tokens,tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use a list comprehension to extract tags from the word/tag combination (the variable `words_tags`) and pass the tags to a frequency distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN : 28.62\n",
      ". : 17.08\n",
      "VERB : 14.45\n",
      "ADP : 9.48\n",
      "PRON : 8.62\n",
      "DET : 7.80\n",
      "ADJ : 5.31\n",
      "ADV : 3.95\n",
      "PRT : 2.24\n",
      "CONJ : 1.58\n",
      "NUM : 0.64\n",
      "X : 0.22\n"
     ]
    }
   ],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in words_tags)\n",
    "tag_fd.most_common()[:15]\n",
    "\n",
    "summ = 0\n",
    "for mc in tag_fd.most_common():\n",
    "    summ += mc[1]\n",
    "for mc in tag_fd.most_common():\n",
    "    print \"%s : %0.2f\"%( mc[0], (mc[1]/summ)*100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation - between the nouns, pepositions, conjunctions, and periods, you've got 45% of the chapter covered. \n",
    "\n",
    "There is an implicit bias, in simple taggers, to tag unknown words as nouns, so that may be the cause for all the nouns. But Lestrygonians is a more earthy, organic, and sensory chapter, so it would make sense that Joyce focuses more on nouns, on things and surroundings, and that those would fill up the chapter. \n",
    "\n",
    "We'll see later on that we can explore those parts of speech tags to see whether they were correct and where they went wrong, when we look at multi-word phrases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing the built-in FreqDist object, we can explore the most common occurrences of various parts of speech. For example, if we create a FreqDist with a tagged version of Lestrygonians, we can filter the most common words by their part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a frequency distribution from POS tag data\n",
    "words_tags_fd = nltk.FreqDist(words_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'was',\n",
       " u'said',\n",
       " u'is',\n",
       " u'be',\n",
       " u'have',\n",
       " u'see',\n",
       " u'are',\n",
       " u'do',\n",
       " u'had',\n",
       " u'know',\n",
       " u'did',\n",
       " u'say',\n",
       " u'get',\n",
       " u'has',\n",
       " u'would',\n",
       " u'coming',\n",
       " u'take',\n",
       " u'must',\n",
       " u'come',\n",
       " u'go',\n",
       " u'going',\n",
       " u'got',\n",
       " u'could',\n",
       " u'asked',\n",
       " u'put',\n",
       " u'used',\n",
       " u'will',\n",
       " u'think',\n",
       " u'walked',\n",
       " u'passed']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_verbs = [wt[0] for (wt, _) in words_tags_fd.most_common() if wt[1] == 'VERB']\n",
    "most_common_verbs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Mr',\n",
       " u'Bloom',\n",
       " u'eyes',\n",
       " u'street',\n",
       " u'hand',\n",
       " u'Flynn',\n",
       " u'man',\n",
       " u'Nosey',\n",
       " u'time',\n",
       " u'way',\n",
       " u'Byrne',\n",
       " u'Davy',\n",
       " u'Must',\n",
       " u'day',\n",
       " u'Mrs',\n",
       " u'something',\n",
       " u'night',\n",
       " u'mouth',\n",
       " u'woman',\n",
       " u'things']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_nouns = [wt[0] for (wt, _) in words_tags_fd.most_common() if wt[1] == 'NOUN']\n",
    "most_common_nouns[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a conditional frequency distribution, we can check on the probability that a particular part of speech will be a particular word - that is, finding the most frequent parts of speech. We do this by tabulating a conditional frequency distribution for two independent variables: the words, and the parts of speech. The conditional frequency distribution can then be queried for all of the words corresponding to a particular part of speech, and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd2 = nltk.ConditionalFreqDist((tag, word.lower()) for (word, tag) in words_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can, for example, print the most common numerical words that appear in the chapter, and we can see that one is the most common, followed by two, three, and five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'one', 34), (u'two', 27), (u'three', 10), (u'five', 9), (u'thousand', 3), (u'six', 3), (u'seven', 1), (u'it\\u2019s', 1), (u'ten', 1), (u'eight', 1), (u'four', 1), (u'middle', 1), (u'devour', 1), (u'110', 1), (u'twentyone', 1), (u'no-one', 1), (u'85', 1)]\n"
     ]
    }
   ],
   "source": [
    "print cfd2['NUM'].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"pos_patterns\"></a>\n",
    "## Patterns in Parts of Speech\n",
    "\n",
    "Things get more interesting when we expand the number of words we're looking at to include phrases. For example, the following loop will look for phrases in the form `<POS1> <POS2> <POS3>` and will print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungry famished gull\n",
      "tall white hats\n",
      "only reliable inkeraser\n",
      "ankle first day\n",
      "poor old sot\n",
      "little brother’s family\n",
      "other old mosey\n",
      "huge high door\n",
      "last broad tunic\n",
      "Few years’ time\n",
      "literary etherial people\n",
      "keep quiet relief\n",
      "little naughty boy\n",
      "warm human plumpness\n",
      "last pagan king\n",
      "sweetish warmish cigarettesmoke\n",
      "vegetarian fine flavour\n",
      "big tour end\n",
      "fresh clean bread\n",
      "eggs fifty years\n",
      "filleted lemon sole\n",
      "Soft warm sticky\n",
      "dry pen signature\n",
      "lovely seaside girls\n",
      "fine fine straw\n",
      "long windy steps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pos_combo(pos1,pos2,pos3):\n",
    "    for i in range(len(words_tags)-2):\n",
    "        if( words_tags[i][1]==pos1 ):\n",
    "            if( words_tags[i+1][1]==pos2 ):\n",
    "                if( words_tags[i+2][1]==pos3 ):\n",
    "                    print ' '.join([words_tags[i][0],words_tags[i+1][0],words_tags[i+2][0]])\n",
    "\n",
    "pos_combo('ADJ','ADJ','NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then why is\n",
      "as well get\n",
      "not even registered\n",
      "always feels complimented\n",
      "Still better tell\n",
      "never once saw\n",
      "then you’d have\n"
     ]
    }
   ],
   "source": [
    "pos_combo('ADV','ADV','VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like getting £\n",
      "inside writing letters\n",
      "in pudding time\n",
      "Like holding water\n",
      "of bloodhued poplin\n",
      "in trickling hallways\n",
      "with sopping sippets\n",
      "of making money\n",
      "with juggling fingers\n"
     ]
    }
   ],
   "source": [
    "pos_combo('ADP','VERB','NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`with sopping sippets` above shows an example of a mislabeled word - `sopping` should be labeled as an adjective, but was mislabeled by a tagger that was indiscriminately labeling \"ing\" words as verbs (getting, writing, holding). Likewise, `pudding` was accidentally tagged as a verb for the same reason. There's also an `-ed` word, `bloodhued`, which was, for some inexplicable reason, tagged as a verb, instead of an adjective.\n",
    "\n",
    "In any case, we see that there is room for improvement, but it's an interesting way to explore the text nevertheless.\n",
    "\n",
    "We already explored what parts of speech are the most common in this text. But suppose we're interested, now, in what combinations of parts of speech are the most common. To do this, we can construct a conditional frequency distribution based on the parts of speech. \n",
    "\n",
    "If we're thinking about three-letter phrases, then we want to tabulate the frequency of particular combinations of three parts of speech show up. We can think of this as a probability distribution of three random variables taking on values from a set of nominal values ('NOUN', 'VERB', etc.). This is equivalent to a three-dimensional space chopped up into bins, with different frequencies occurring in each bin based on the words appearing in the text.\n",
    "\n",
    "However, because NLTK's built-in conditional frequency object is only designed to handle two-dimensional data, we'll have to be a bit careful. We'll start by picking the first part of speech (thus eliminating one variable). Then we'll tabulate the conditional frequencies the combinations of parts of speech for the remaining two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        .  ADJ  ADP  ADV CONJ  DET NOUN  NUM PRON  PRT VERB    X \n",
      "   .    0    8    7    7    0   10   33    0    5    0   18    1 \n",
      " ADJ    4    2    1    1    0    1   26    1    0    0    1    0 \n",
      " ADP    2    0    2    1    0   15   11    0   16    0    2    0 \n",
      " ADV   16    2    1    0    0    0    1    0    1    1    3    0 \n",
      "CONJ    0   12    0    0    0    0    1    0    0    0    0    0 \n",
      " DET    0    1    0    0    0    0    4    0    0    0    0    0 \n",
      "NOUN  233   10   95   15   16   10   62    2   26   13   56    1 \n",
      " NUM    2    0    0    0    0    0    4    0    0    0    1    0 \n",
      "PRON    0    0    0    0    0    0    0    0    0    0   10    0 \n",
      " PRT    1    0    0    0    0    0    0    0    0    0    3    0 \n",
      "VERB    3    1    4    2    0    4    4    1    1    2    3    0 \n",
      "   X    0    0    0    0    0    0    0    0    0    1    0    0 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_trigrams(tagged_words):\n",
    "    for i in range(len(tagged_words)-2):\n",
    "        yield (tagged_words[i],tagged_words[i+1],tagged_words[i+2])\n",
    "\n",
    "trigram_freq = {}\n",
    "for ((word1,tag1),(word2,tag2),(word3,tag3)) in get_trigrams(words_tags):    \n",
    "    if( tag1 in trigram_freq.keys() ):\n",
    "        trigram_freq[tag1].append((tag2,tag3))\n",
    "    else:\n",
    "        trigram_freq[tag1] = [(tag2,tag3)]\n",
    "\n",
    "adj_cf = nltk.ConditionalFreqDist(trigram_freq['ADJ'])\n",
    "print adj_cf.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dense, interesting table. This table shows the likelihood of particular parts of speech occurring after adjectives. \n",
    "\n",
    "To utilize this table, we begin by selecting a part of speech (in this case, adjective) that is the basis for the table. Next, we pick the part of speech of the second word, and select it from the labels on the left side of the table. (That is, the rows indicate the part of speech of the second word in our trigram). Finally, we pick the part of speech of the third word, and select it from the labels on the top of the table. (The columns indicate the part of speech fo the third word in our trigram.)\n",
    "\n",
    "This gives the total occurrences of this combination of parts of speech.\n",
    "\n",
    "The first thing we notice is that nouns are by far the most common part of speech to occur after adjectives - precisely what we would expect. Verbs are far less common as the second word in our trigram - the adjective-verb combination is rather unusual. But verbs are much more common as the third part of speech in the trigram.\n",
    "\n",
    "Suppose we're interested in the combination `adjective-pronoun-verb`, which the table tells us occurs exactly 10 times. We can print out these combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priest they are\n",
      "inkbottle I suggested\n",
      "sure she was\n",
      "nun they say\n",
      "flat they look\n",
      "Lucky I had\n",
      "Italian I prefer\n",
      "ready he drained\n",
      "green it would\n",
      "First I must\n"
     ]
    }
   ],
   "source": [
    "pos_combo('ADJ','PRON','VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `adjective-pronoun-verb` combination seems to happen when there is a change or transition in what the sentence is saying - the beginning of a new phrase.\n",
    "\n",
    "On the other hand, if we look at the `adjective-noun-adjective` combination, which is similarly infrequent, it shows words that look like they were intended to cluster together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty yards astern\n",
      "old mosey lunatic\n",
      "big establishments whole\n",
      "right royal old\n",
      "casual wards full\n",
      "fifty years old\n",
      "full lips full\n",
      "woman’s breasts full\n",
      "Lean people long\n",
      "biliary duct spleen\n"
     ]
    }
   ],
   "source": [
    "pos_combo('ADJ','NOUN','ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Pineapple rock , lemon platt , butter scotch...>\n",
      "['_CONTEXT_RE', '_COPY_TOKENS', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__len__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_context', '_word_context_index', 'collocations', 'common_contexts', 'concordance', 'count', 'dispersion_plot', 'findall', 'index', 'name', 'plot', 'readability', 'similar', 'tokens', 'unicode_repr', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "print sanetext\n",
    "print dir(sanetext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal more deftly with this information, and actually make good use of it, we would need to get a little more sohpisticated with how we're storing our independent variables. \n",
    "\n",
    "This information could be used to reveal what combinations of parts of speech are most likely to be complete, standalone phrases. For example, the adjective-pronoun-verb combination does not result in words that are intended to work together in a single phrase. If we were to analyze three-word phrases beginning with pronouns, for example, we would see that pronouns followed by verbs are very common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        .  ADJ  ADP  ADV CONJ  DET NOUN  NUM PRON  PRT VERB    X \n",
      "   .    3    9   14    3    3    8   69    2   21    1   37    1 \n",
      " ADJ    4    5    0    1    2    0   55    0    0    0    0    0 \n",
      " ADP    5    4    9    1    1   28   22    0   17    0    5    0 \n",
      " ADV   15    1    4    2    1    1    0    0    1    2   13    0 \n",
      "CONJ    0    0    0    0    0    0    1    0    1    0    1    0 \n",
      " DET    5    8    3    1    0    1   11    0    0    0    5    0 \n",
      "NOUN  138    1   34   16   15    6   23    1    7   11   61    0 \n",
      " NUM    0    3    0    0    0    0    1    0    0    0    0    0 \n",
      "PRON    1    1    1    1    0    0    1    0    1    0   15    0 \n",
      " PRT    9    2   10    1    2    4    4    0    5    0    4    0 \n",
      "VERB  109   19   56   47    1   56   29    1   96   34   72    0 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pro_cf = nltk.ConditionalFreqDist(trigram_freq['PRON'])\n",
    "print pro_cf.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the pronoun-verb combination occurs frequently, but the adjective-pronoun-verb combination does not, indicates that the adjective-pronoun-verb combination is unlikely to be a sensible phrase. By analyzing a few specific sentences and identifying issues with tags, as we did above, it's possible to improve the part of speech tagger. It's also possible to use a hierarchical part of speech tagger - one that starts by looking at three or four neighboring words, and determing the part of speech based on their part of speech. If that fails, it looks at two neighboring words, or one neighboring word, until the simplest case, when it looks at a single word itself. Many of the part of speech tags that failed were fooled by suffixes of words, and ignored their context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"pos_improving\"></a>\n",
    "## Improving Part of Speech Tagging\n",
    "\n",
    "To improve the tagging of parts of speech, we can use n-gram tagging, which is the idea that when you're tagging a word with a part of speech, it can be helpful to look at neighboring words and their parts of speech. However, in order to do this, we'll need a set of training data, to train our part of speech tagger.\n",
    "\n",
    "Start by importing a corpus. The `brown` corpus, described at the [Brown Corpus wikipedia page](https://en.wikipedia.org/wiki/Brown_Corpus), is approximately one million tagged English words, in a range of different categories. This is far more complete than the `treebank` corpus, another tagged (but partially complete) corpus from the University of Pennsylvania (the Treebank project's homepage gives a 404, but see [Treebank-3](https://catalog.ldc.upenn.edu/LDC99T42) from the Linguistic Data Consortium). \n",
    "\n",
    "We'll use two data sets: one training data set, to train the part of speech tagger, using tagged sentences; and one test data set, to which we apply our part of speech tagger, and compare to a known result (tagged versions of the same sentences), enabling a quantitative measure of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__name__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_add', '_get_root', '_init', '_resolve', 'abspath', 'abspaths', 'categories', 'citation', 'encoding', 'ensure_loaded', 'fileids', 'license', 'open', 'paras', 'raw', 'readme', 'root', 'sents', 'tagged_paras', 'tagged_sents', 'tagged_words', 'unicode_repr', 'words']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print dir(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get tagged and untagged sentences of fiction\n",
    "btagsent = brown.tagged_sents(categories='fiction')\n",
    "bsent = brown.sents(categories='fiction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data into two parts: the training data set and the test data set. The [NLTK book](http://www.nltk.org/book/ch05.html) suggests 90%/10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = 0.90          # z is the test/train ratio\n",
    "omz = 1 - z       # one minus z\n",
    "traincutoff = int(len(btagsent)*z)\n",
    "\n",
    "traindata = btagsent[:traincutoff]\n",
    "testdata = btagsent[traincutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import timeit so we can time how long it takes to train and test POS tagger\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824455652602\n",
      "1.16657710075\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# NLTK makes training and testing really easy\n",
    "unigram_tagger = nltk.UnigramTagger(traindata)\n",
    "perf1 = unigram_tagger.evaluate(testdata)\n",
    "\n",
    "time1 = timeit.default_timer() - start_time\n",
    "\n",
    "print perf1\n",
    "print time1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different taggers can be combined, in a certain order, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86615819904\n",
      "2.55090999603\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(traindata, backoff=t0)\n",
    "t2 = nltk.BigramTagger(traindata, backoff=t1)\n",
    "perf2 = t2.evaluate(testdata)\n",
    "\n",
    "time2 = timeit.default_timer() - start_time\n",
    "\n",
    "print perf2\n",
    "print time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting improvement: 5 %\n",
      "Timing penalty: 118 %\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting improvement: %d %%\"%(100*abs(perf2-perf1)/perf1)\n",
    "print \"Timing penalty: %d %%\"%(100*abs(time2-time1)/time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm......\n",
    "\n",
    "In any case - if we want to save this model, we can save it in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "output = open('lestrygonians_parser.pkl', 'wb')\n",
    "dump(t2, output, -1)\n",
    "output.close()\n",
    "\n",
    "## To load:\n",
    "# from pickle import load\n",
    "# input = open('lestrygonians_parser.pkl', 'rb')\n",
    "# tagger = load(input)\n",
    "# input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                original\t\t                        better\n",
      "                          Pineapple (NOUN)\t                          Pineapple (NN)\n",
      "                               rock (NOUN)\t                               rock (NN)\n",
      "                                  , (.)\t                                  , (,)\n",
      "                              lemon (ADJ)\t                              lemon (NN)\n",
      "                              platt (NOUN)\t                              platt (NN)\n",
      "                                  , (.)\t                                  , (,)\n",
      "                             butter (NOUN)\t                             butter (NN)\n",
      "                             scotch (NOUN)\t                             scotch (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                  A (DET)\t                                  A (AT)\n",
      "                        sugarsticky (ADJ)\t                        sugarsticky (NN)\n",
      "                               girl (NOUN)\t                               girl (NN)\n",
      "                         shovelling (VERB)\t                         shovelling (NN)\n",
      "                          scoopfuls (NOUN)\t                          scoopfuls (NN)\n",
      "                                 of (ADP)\t                                 of (IN)\n",
      "                             creams (NOUN)\t                             creams (NN)\n",
      "                                for (ADP)\t                                for (IN)\n",
      "                                  a (DET)\t                                  a (AT)\n",
      "                          christian (ADJ)\t                          christian (NN)\n",
      "                            brother (NOUN)\t                            brother (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                               Some (DET)\t                               Some (DTI)\n",
      "                             school (NOUN)\t                             school (NN)\n",
      "                              treat (NOUN)\t                              treat (VB)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                Bad (NOUN)\t                                Bad (NN)\n",
      "                                for (ADP)\t                                for (IN)\n",
      "                              their (PRON)\t                              their (PP$)\n",
      "                            tummies (NOUN)\t                            tummies (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                            Lozenge (NOUN)\t                            Lozenge (NN)\n",
      "                                and (CONJ)\t                                and (CC)\n",
      "                             comfit (VERB)\t                             comfit (NN)\n",
      "                       manufacturer (NOUN)\t                       manufacturer (NN)\n",
      "                                 to (PRT)\t                                 to (TO)\n",
      "                                His (PRON)\t                                His (PP$)\n",
      "                            Majesty (NOUN)\t                            Majesty (NN)\n",
      "                                the (DET)\t                                the (AT)\n",
      "                               King (NOUN)\t                               King (NN-TL)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                God (NOUN)\t                                God (NP)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                               Save (NOUN)\t                               Save (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                Our (PRON)\t                                Our (PP$-TL)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                            Sitting (VERB)\t                            Sitting (VBG)\n",
      "                                 on (ADP)\t                                 on (IN)\n",
      "                                his (PRON)\t                                his (PP$)\n",
      "                             throne (NOUN)\t                             throne (NN)\n",
      "                            sucking (NOUN)\t                            sucking (VBG)\n",
      "                                red (ADJ)\t                                red (JJ)\n",
      "                            jujubes (ADJ)\t                            jujubes (NN)\n",
      "                              white (ADJ)\t                              white (JJ)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                  A (DET)\t                                  A (AT)\n",
      "                             sombre (ADJ)\t                             sombre (NN)\n",
      "                            Y.M.C.A (NOUN)\t                            Y.M.C.A (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                              young (ADJ)\t                              young (JJ)\n",
      "                                man (NOUN)\t                                man (NN)\n",
      "                                  , (.)\t                                  , (,)\n",
      "                           watchful (ADJ)\t                           watchful (NN)\n",
      "                              among (ADP)\t                              among (IN)\n",
      "                                the (DET)\t                                the (AT)\n",
      "                               warm (ADJ)\t                               warm (JJ)\n",
      "                              sweet (NOUN)\t                              sweet (JJ)\n",
      "                              fumes (NOUN)\t                              fumes (NNS)\n",
      "                                 of (ADP)\t                                 of (IN)\n",
      "                             Graham (NOUN)\t                             Graham (NN)\n",
      "                            Lemon’s (NOUN)\t                            Lemon’s (NN)\n",
      "                                  , (.)\t                                  , (,)\n",
      "                             placed (VERB)\t                             placed (VBD)\n",
      "                                  a (DET)\t                                  a (AT)\n",
      "                          throwaway (NOUN)\t                          throwaway (NN)\n",
      "                                 in (ADP)\t                                 in (IN)\n",
      "                                  a (DET)\t                                  a (AT)\n",
      "                               hand (NOUN)\t                               hand (NN)\n",
      "                                 of (ADP)\t                                 of (IN)\n",
      "                                 Mr (NOUN)\t                                 Mr (NN)\n",
      "                              Bloom (NOUN)\t                              Bloom (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                              Heart (NOUN)\t                              Heart (NN-TL)\n",
      "                                 to (PRT)\t                                 to (TO)\n",
      "                              heart (NOUN)\t                              heart (NN)\n",
      "                              talks (NOUN)\t                              talks (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                               Bloo (NOUN)\t                               Bloo (NN)\n",
      "                                ... (.)\t                                ... (NN)\n",
      "                                 Me (NOUN)\t                                 Me (NN)\n",
      "                                  ? (.)\t                                  ? (.)\n",
      "                                 No (NOUN)\t                                 No (RB)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                              Blood (NOUN)\t                              Blood (NN)\n",
      "                                 of (ADP)\t                                 of (IN)\n",
      "                                the (DET)\t                                the (AT)\n",
      "                               Lamb (NOUN)\t                               Lamb (NN)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                His (PRON)\t                                His (PP$)\n",
      "                               slow (ADJ)\t                               slow (JJ)\n",
      "                               feet (NOUN)\t                               feet (NNS)\n",
      "                             walked (VERB)\t                             walked (VBD)\n",
      "                                him (PRON)\t                                him (PPO)\n",
      "                          riverward (NOUN)\t                          riverward (NN)\n",
      "                                  , (.)\t                                  , (,)\n",
      "                            reading (NOUN)\t                            reading (VBG)\n",
      "                                  . (.)\t                                  . (.)\n",
      "                                Are (NOUN)\t                                Are (BER)\n",
      "                                you (PRON)\t                                you (PPSS)\n",
      "                              saved (VERB)\t                              saved (VBN)\n"
     ]
    }
   ],
   "source": [
    "better_tags = t2.tag(tokens)\n",
    "\n",
    "print \"%40s\\t\\t%30s\"%(\"original\",\"better\")\n",
    "for z in range(110):\n",
    "    print \"%35s (%s)\\t%35s (%s)\"%(words_tags[z][0],words_tags[z][1],better_tags[z][0],better_tags[z][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The analysis, from here, moves on to phrases and sentences, which will be covered in Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
