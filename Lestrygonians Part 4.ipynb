{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Ulysses with NLTK: Lestrygonians (Ch. 8)\n",
    "\n",
    "## Part IV: Wordplay\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "### Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Tokenizing Without Punctuation](#tokenizing_wo_punctuation)\n",
    "* [Method 1: TokenSearcher Object](#tokensearcher)\n",
    "* [Method 2: Bigram Splitting Method](#bigram_splitting)\n",
    "* [Functionalizing Bigram Search Methods](#functionalizing)\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll analyze some of Joyce's wordplay in Ulysses, using more complicated regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tokenizing_wo_punctuation\"></a>\n",
    "## Tokenizing Without Punctuation\n",
    "\n",
    "To tokenize the chapter and throw out the punctuation, we can use the regular expression `\\w+`. Note that this will split up contractions like \"can't\" into `[\"can\",\"t\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk, re, io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'really', u'It', u's', u'always', u'flowing', u'in', u'a', u'stream', u'never', u'the', u'same', u'which', u'in', u'the', u'stream', u'of', u'life', u'we', u'trace', u'Because']\n"
     ]
    }
   ],
   "source": [
    "txtfile = 'txt/08lestrygonians.txt'\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "with io.open(txtfile) as f:\n",
    "    tokens = tokenizer.tokenize(f.read())\n",
    "print tokens[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 't', 'keep', 'a', 'contraction', 'together']\n"
     ]
    }
   ],
   "source": [
    "print tokenizer.tokenize(\"can't keep a contraction together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tokensearcher\"></a>\n",
    "## Method 1: TokenSearcher Object\n",
    "\n",
    "The first method for searching for regular expressions in a set of tokens is the TokenSearcher object. This can be fed a regular expression that searches across tokens, and it will search through each token. This provides a big advantage: we don't have to manually break all of our tokens into n-grams ourselves, we can just let the TokenSearcher do the hard work. \n",
    "\n",
    "Here's an example of how to create and call that object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "scotch A sugarsticky girl shovelling\n",
      "selling off some old furniture\n",
      "saw flapping strongly wheeling between\n",
      "seabirds gulls seagoose Swans from\n",
      "sound She s not exactly\n",
      "sandwichmen marched slowly towards him\n",
      "street after street Just keep\n",
      "smart girls sitting inside writing\n",
      "suited her small head Sister\n",
      "she If she had married\n",
      "some sticky stuff Flies picnic\n",
      "she looked soaped all over\n",
      "saint Kevin s parade Pen\n",
      "s womaneyes said melancholily Now\n",
      "said He s a caution\n",
      "said He s always bad\n",
      "speak Look straight in her\n",
      "serge dress she had two\n",
      "sugary flour stuck to her\n",
      "simply Child s head too\n",
      "something to stop that Life\n",
      "she had so many children\n",
      "said The spoon of pap\n",
      "still All skedaddled Why he\n",
      "squad Turnkey s daughter got\n",
      "sun slowly shadowing Trinity s\n",
      "say Other steps into his\n",
      "spewed Provost s house The\n",
      "s uniform since he got\n",
      "say it s healthier Windandwatery\n",
      "sweating Irish stew into their\n",
      "s daughter s bag and\n",
      "some king s mistress His\n",
      "s bottle shoulders On his\n",
      "street west something changed Could\n",
      "s corner still pursued Jingling\n",
      "shovelled gurgling soup down his\n",
      "stewgravy with sopping sippets of\n",
      "server gathered sticky clattering plates\n",
      "second helping stared towards the\n",
      "split their skulls open Moo\n",
      "sheepsnouts bloodypapered snivelling nosejam on\n",
      "smokinghot thick sugary Famished ghosts\n",
      "something the somethings of the\n",
      "some fellow s digestion Religions\n",
      "sandwich Yes sir Like a\n",
      "s the style Who s\n",
      "sandwich into slender strips Mr\n",
      "see Part shares and part\n",
      "strongly to speed it set\n",
      "said He s the organiser\n",
      "snuffled and scratched Flea having\n",
      "such and such replete Too\n",
      "strips of sandwich fresh clean\n",
      "s no straight sport going\n",
      "soaked and softened rolled pith\n",
      "sturgeon high sheriff Coffey the\n",
      "soup Geese stuffed silly for\n",
      "s the same fish perhaps\n",
      "sky No sound The sky\n",
      "see Never speaking I mean\n",
      "something fall see if she\n",
      "said They stick to you\n",
      "said He s a safe\n",
      "say He s not too\n",
      "sake What s yours Tom\n",
      "said Certainly sir Paddy Leonard\n",
      "said with scorn Mr Byrne\n",
      "said A suckingbottle for the\n",
      "sweet then savoury Mr Bloom\n",
      "s confectioner s window of\n",
      "said Molesworth street is opposite\n",
      "street different smell Each person\n",
      "spring the summer smells Tastes\n",
      "shameless not seeing That girl\n",
      "school I sentenced him to\n",
      "sunlight Tan shoes Turnedup trousers\n",
      "stuck Ah soap there I\n"
     ]
    }
   ],
   "source": [
    "tsearch = nltk.TokenSearcher(tokens)\n",
    "s_s_ = tsearch.findall(r'<s.*> <.*> <s.*> <.*> <.*>')\n",
    "print len(s_s_)\n",
    "for s in s_s_:\n",
    "    print ' '.join(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bigram_splitting\"></a>\n",
    "## Method 2: Bigram Splitting Method\n",
    "\n",
    "Another way of searching for patterns, one that may be needed if we want to use criteria that would be hard to implement with a regular expression (such as finding two words that are the same length next to each other), is to assemble all of the tokens into bigrams.\n",
    "\n",
    "Suppose we are looking for two words that start with the same letter. We can do this by iterating through a set of bigrams (we'll use a built-in NLTK object to generate bigrams), and apply our search criteria to the first and second words independently. \n",
    "\n",
    "To create bigrams, we'll use the `nltk.bigrams()` method, feeding it a list of tokens.\n",
    "\n",
    "When we do this, we can see there's a lot of alliteration in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printlist(the_list):\n",
    "    for item in the_list:\n",
    "        print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 551 pairs of words starting with the same letter:\n",
      "shovelling scoopfuls\n",
      "their tummies\n",
      "riverward reading\n",
      "wife will\n",
      "to the\n",
      "in it\n",
      "he had\n",
      "their theology\n",
      "out of\n",
      "themselves to\n",
      "funds for\n",
      "to the\n",
      "it is\n",
      "short sighs\n",
      "cream curves\n",
      "hasty hand\n",
      "Agendath Afternoon\n",
      "she said\n",
      "Potato Purse\n",
      "his hip\n"
     ]
    }
   ],
   "source": [
    "alliteration = []\n",
    "for (i,j) in nltk.bigrams(tokens):\n",
    "    if i[:1]==j[:1]:\n",
    "        alliteration.append( ' '.join([i,j]) )\n",
    "        \n",
    "\n",
    "print \"Found\",len(alliteration),\"pairs of words starting with the same letter:\"\n",
    "printlist(alliteration[:10])\n",
    "printlist(alliteration[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107 pairs of words, one containing 'll' and the other containing 'l':\n",
      "First 25:\n",
      "girl shovelling\n",
      "shovelling scoopfuls\n",
      "All heartily\n",
      "like all\n",
      "himself well\n",
      "collie floating\n",
      "quaywalls gulls\n",
      "ball Elijah\n",
      "swells floated\n",
      "gull Flaps\n",
      "treacly swells\n",
      "swells lazily\n",
      "parallel parallax\n",
      "all Only\n",
      "black celluloid\n",
      "envelopes Hello\n",
      "Kansell sold\n",
      "Phil Gilligan\n",
      "Val Dillon\n",
      "flag fell\n",
      "wallpaper Dockrell\n",
      "probably Well\n",
      "blizzard collar\n",
      "gaily Milly\n",
      "medicinebottle Pastille\n"
     ]
    }
   ],
   "source": [
    "lolly = []\n",
    "for (i,j) in nltk.bigrams(tokens):\n",
    "    if len( re.findall('ll',i) )>0:\n",
    "        if len( re.findall('l',j) )>0:\n",
    "            lolly.append( ' '.join([i,j]) )\n",
    "    elif len( re.findall('ll',j) )>0:\n",
    "        if len( re.findall('l',i) )>0:\n",
    "            lolly.append(' '.join([i,j]) )\n",
    "\n",
    "print \"Found\",len(lolly),\"pairs of words, one containing 'll' and the other containing 'l':\"\n",
    "print \"First 25:\"\n",
    "printlist(lolly[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 pairs of words, one containing 'r' and the other containing 'r':\n",
      "daguerreotype atelier\n",
      "supperroom or\n",
      "from Harrison\n",
      "terrible for\n",
      "Farrell Mr\n",
      "Weightcarrying huntress\n",
      "or fivebarred\n",
      "Dr Murren\n",
      "marching irregularly\n",
      "irregularly rounded\n",
      "suburbs jerrybuilt\n",
      "jerrybuilt Kerwan\n",
      "garden Terrific\n",
      "Portobello barracks\n",
      "artificial irrigation\n",
      "irrigation Bleibtreustrasse\n",
      "dropping currants\n",
      "currants Screened\n",
      "whispered Prrwht\n",
      "ravenous terrier\n",
      "Earlsfort terrace\n",
      "Where Hurry\n"
     ]
    }
   ],
   "source": [
    "lolly = []\n",
    "for (i,j) in nltk.bigrams(tokens):\n",
    "    if len( re.findall('rr',i) )>0:\n",
    "        if len( re.findall('r',j) )>0:\n",
    "            lolly.append( ' '.join([i,j]) )\n",
    "    elif len( re.findall('rr',j) )>0:\n",
    "        if len( re.findall('r',i) )>0:\n",
    "            lolly.append(' '.join([i,j]) )\n",
    "\n",
    "print \"Found\",len(lolly),\"pairs of words, one containing 'r' and the other containing 'r':\"\n",
    "printlist(lolly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"functionalizing\"></a>\n",
    "## Functionalizing Bigram Searches\n",
    "\n",
    "We can functionalize the search for patterns with a single and double character shared, i.e., `dropping currants` (the letter r).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def double_letter_alliteration(c,tokens):\n",
    "    \"\"\"\n",
    "    This function finds all occurrences of double-letter and single-letter \n",
    "    occurrences of the character c.\n",
    "    \n",
    "    This function is called by all_double_letter_alliteration().\n",
    "    \"\"\"\n",
    "    allall  = []\n",
    "    for (i,j) in nltk.bigrams(tokens):\n",
    "        if len( re.findall(c+c,i) )>0:\n",
    "            if len( re.findall(c,j) )>0:\n",
    "                lolly.append( ' '.join([i,j]) )\n",
    "        elif len( re.findall(c+c,j) )>0:\n",
    "            if len( re.findall(c,i) )>0:\n",
    "                allall.append(' '.join([i,j]) )\n",
    "    return allall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to search for the single-double letter pattern individually, or we can define a function that will loop over all 26 letters to find all matching patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from Harrison\n",
      "or fivebarred\n",
      "Dr Murren\n",
      "marching irregularly\n",
      "suburbs jerrybuilt\n",
      "garden Terrific\n",
      "Portobello barracks\n",
      "artificial irrigation\n",
      "dropping currants\n",
      "whispered Prrwht\n",
      "ravenous terrier\n",
      "Earlsfort terrace\n",
      "Where Hurry\n"
     ]
    }
   ],
   "source": [
    "printlist(double_letter_alliteration('r',tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shovelling scoopfuls\n",
      "Some school\n",
      "No Blood\n",
      "word Good\n",
      "story too\n",
      "lower Looking\n",
      "Those poor\n",
      "Trousers Good\n",
      "to loosen\n",
      "Women too\n",
      "People looking\n",
      "photography Poor\n",
      "or oakroom\n",
      "Professor Goodwin\n",
      "old Goodwin\n",
      "now Cook\n",
      "Pothunters too\n",
      "Tommy Moore\n",
      "mortarboards Looking\n",
      "corporation too\n",
      "of goosegrease\n",
      "two loonies\n",
      "of food\n",
      "Women too\n",
      "money too\n",
      "from School\n",
      "you poor\n",
      "Molly looks\n",
      "of bloodhued\n",
      "lustrous blood\n",
      "sloppy food\n",
      "Working tooth\n",
      "to look\n",
      "own tooth\n",
      "onions mushrooms\n",
      "open Moo\n",
      "sheepsnouts bloodypapered\n",
      "Hello Bloom\n",
      "missionary too\n",
      "olives too\n",
      "Not logwood\n",
      "of wood\n",
      "some good\n",
      "more Fool\n",
      "for food\n",
      "of Moore\n",
      "wonder Coolsoft\n",
      "gods food\n",
      "not too\n",
      "down too\n",
      "some bloody\n",
      "of poor\n",
      "Horse drooping\n",
      "stronger too\n",
      "person too\n",
      "Not smooth\n",
      "to Poor\n",
      "bluecoat school\n",
      "pocket took\n"
     ]
    }
   ],
   "source": [
    "printlist(double_letter_alliteration('o',tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def all_double_letter_alliteration(tokens):\n",
    "    all_all = []\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    for aleph in alphabet:\n",
    "        results = double_letter_alliteration(aleph,tokens) \n",
    "        print \"Matching\",aleph,\":\",len(results)\n",
    "        all_all += results\n",
    "    return all_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching a : 1\n",
      "Matching b : 3\n",
      "Matching c : 4\n",
      "Matching d : 8\n",
      "Matching e : 109\n",
      "Matching f : 1\n",
      "Matching g : 5\n",
      "Matching h : 1\n",
      "Matching i : 0\n",
      "Matching j : 0\n",
      "Matching k : 0\n",
      "Matching l : 47\n",
      "Matching m : 1\n",
      "Matching n : 16\n",
      "Matching o : 59\n",
      "Matching p : 1\n",
      "Matching q : 0\n",
      "Matching r : 13\n",
      "Matching s : 31\n",
      "Matching t : 38\n",
      "Matching u : 0\n",
      "Matching v : 0\n",
      "Matching w : 0\n",
      "Matching x : 0\n",
      "Matching y : 0\n",
      "Matching z : 0\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "allall = all_double_letter_alliteration(tokens)\n",
    "print len(allall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a mouthful of alliteration! We can compare the number of words that matched this (one, single) search for examples of alliteration to the total number of words in the chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026187340202990624"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double(len(allall))/len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy cow - 2.6% of the chapter is just this one alliteration pattern, of having two neighbor words: one with a double letter, and one with a single letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "bawling maaaaaa\n",
      "ball bobbed\n",
      "bob Bubble\n",
      "buckets wobbly\n",
      "collecting accounts\n",
      "Scotch accent\n",
      "Scotch accent\n",
      "crown Accept\n",
      "had plodded\n",
      "dumdum Diddlediddle\n",
      "and bidding\n",
      "remembered Hidden\n",
      "naked goddesses\n",
      "said Paddy\n",
      "standing Paddy\n",
      "Rochford nodded\n",
      "bluey greeny\n",
      "goes Fifteen\n",
      "they feel\n",
      "They wheeled\n"
     ]
    }
   ],
   "source": [
    "print len(allall)\n",
    "printlist(allall[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the pattern taken one step further: we'll look for double letters in neighbor words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor words with double letters:\n",
      "Matching aa: 0\n",
      "Matching bb: 0\n",
      "Matching cc: 0\n",
      "Matching dd: 0\n",
      "Matching ee: 5\n",
      "Matching ff: 2\n",
      "Matching gg: 0\n",
      "Matching hh: 0\n",
      "Matching ii: 0\n",
      "Matching jj: 0\n",
      "Matching kk: 0\n",
      "Matching ll: 15\n",
      "Matching mm: 0\n",
      "Matching nn: 2\n",
      "Matching oo: 4\n",
      "Matching pp: 3\n",
      "Matching qq: 0\n",
      "Matching rr: 0\n",
      "Matching ss: 1\n",
      "Matching tt: 1\n",
      "Matching uu: 0\n",
      "Matching vv: 0\n",
      "Matching ww: 0\n",
      "Matching xx: 0\n",
      "Matching yy: 0\n",
      "Matching zz: 0\n",
      "wheeling between\n",
      "Fleet street\n",
      "Three cheers\n",
      "greens See\n",
      "green cheese\n",
      "scruff off\n",
      "sheriff Coffey\n",
      "quaywalls gulls\n",
      "parallel parallax\n",
      "wallpaper Dockrell\n",
      "Tisdall Farrell\n",
      "belly swollen\n",
      "still All\n",
      "Silly billies\n",
      "ll tell\n",
      "swollen belly\n",
      "Wellmannered fellow\n",
      "ball falls\n",
      "full All\n",
      "Kill Kill\n",
      "numbskull Will\n",
      "William Miller\n",
      "Penny dinner\n",
      "canny Cunning\n",
      "looks too\n",
      "Goosestep Foodheated\n",
      "loonies mooching\n",
      "Moo Poor\n",
      "Happy Happier\n",
      "Happy Happy\n",
      "sopping sippets\n",
      "pressed grass\n",
      "platt butter\n"
     ]
    }
   ],
   "source": [
    "def match_double(aleph,tokens):\n",
    "    matches = []\n",
    "    for (i,j) in nltk.bigrams(tokens):\n",
    "        if len( re.findall(aleph+aleph,i) )>0:\n",
    "            if len( re.findall(aleph+aleph,j) )>0:\n",
    "                matches.append(' '.join([i,j]))\n",
    "    return matches\n",
    "\n",
    "def double_double(tokens):\n",
    "    dd = []\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    for aleph in alphabet:\n",
    "        results = match_double(aleph, tokens)\n",
    "        print \"Matching %s%s: %d\"%(aleph,aleph,len(results))\n",
    "        dd += results\n",
    "    return dd\n",
    "\n",
    "print \"Neighbor words with double letters:\"\n",
    "dd = double_double(tokens)\n",
    "printlist(dd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"acronyms\"></a>\n",
    "## Acronyms\n",
    "\n",
    "Let's take a look at some acronyms. For this application, it might be better to tokenize by sentence, and extract acronyms for sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n"
     ]
    }
   ],
   "source": [
    "with io.open(txtfile) as f:\n",
    "    sentences = nltk.sent_tokenize(f.read())\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n",
      "--------------------\n",
      "Prlpbs\n",
      "Asgssocfacb\n",
      "Sst\n",
      "Bftt\n",
      "LacmtHMtK\n",
      "G\n",
      "S\n",
      "O\n",
      "Sohtsrjw\n",
      "AsY\n",
      "--------------------\n",
      "Pineapple rock, lemon platt, butter scotch.\n",
      "A sugarsticky girl\n",
      "shovelling scoopfuls of creams for a christian brother.\n",
      "Some school\n",
      "treat.\n",
      "Bad for their tummies.\n",
      "Lozenge and comfit manufacturer to His\n",
      "Majesty the King.\n",
      "God.\n",
      "Save.\n",
      "Our.\n",
      "Sitting on his throne sucking red\n",
      "jujubes white.\n",
      "A sombre Y.M.C.A.\n"
     ]
    }
   ],
   "source": [
    "acronyms = []\n",
    "for s in sentences:\n",
    "    s2 = re.sub('\\n',' ',s)\n",
    "    words = s2.split(\" \")\n",
    "    acronym = ''.join(w[0] for w in words if w<>u'')\n",
    "    acronyms.append(acronym)\n",
    "            \n",
    "print len(acronyms)\n",
    "print \"-\"*20\n",
    "printlist(acronyms[:10])\n",
    "print \"-\"*20\n",
    "printlist(sentences[:10]) # <-- contains newlines, but removed to create acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Khttast',\n",
       " u'Twl',\n",
       " u'Lfg',\n",
       " u'W',\n",
       " u'Htdatacpb',\n",
       " u'Etfpsic',\n",
       " u'Nab',\n",
       " u'Tbbuotwosfubtb',\n",
       " u'Nsdf',\n",
       " u'AtdIttscootEKpiuitwfya']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acronyms[101:111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
